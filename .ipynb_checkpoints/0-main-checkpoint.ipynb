{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a701e748",
   "metadata": {},
   "source": [
    "TO DO\n",
    "\n",
    "general implementation\n",
    "    create a graph or diagram to tell whats hapenning\n",
    "    mark all the tasks/steps complete, incomplete, in progress, problems, to do, research etc.\n",
    "    try on a toy problem\n",
    "\n",
    "find new datasets\n",
    "    for different datasets different preprocessing techniques should be applied\n",
    "    RCV1-V2\n",
    "decide on splitting ratio 20 60 20 \n",
    "\n",
    "try different similarity measures \n",
    "    reference paper\n",
    "    cosine\n",
    "    euclidean\n",
    "    \n",
    "adding AL to determine a reference point for the dataset\n",
    "to find similar instances maybe asking to the user to choose some reference point, using these reference instances to calculate similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b372be",
   "metadata": {},
   "source": [
    "implementation steps\n",
    "\n",
    "+1. reading data and preprocessing\n",
    "2. vectorization\n",
    "    -2.1 embeddings - will try other embeddings, and will search which one is best for datasets\n",
    "    -2.2 dimensionality reduction? (is similarity more accurate when dim. red. done)  - research\n",
    "3. initial classifier to show results\n",
    "4. calculate imbalance ratio and find the ratio of newly labeled data\n",
    "5. oversample dataset using unlabeled set\n",
    "    5.1 find the proper similarity function (eclidean, cosine etc.)\n",
    "        Measurement of Text Similarity: A Survey: a very detailed survey of similarity functions that are used for text data\n",
    "        https://www.kdnuggets.com/2019/01/comparison-text-distance-metrics.html\n",
    "        cosine similarity\n",
    "        minkowski family (euclidean, manhattan)\n",
    "        hamming distance\n",
    "        Jaccard index\n",
    "        Sorensen-dice index\n",
    "        KL divergence\n",
    "        Jensen–Shannon divergence with LDA\n",
    "        Wasserstein distance\n",
    "        SMTP \n",
    "        word mover’s distance\n",
    "    5.2 define a threshold or mechanism to add data for multilabeled set\n",
    "6. train a final classifier to compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "important-accent",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import utilities\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decimal-packing",
   "metadata": {},
   "source": [
    "## parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "hundred-reading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# algorithm parameters\n",
    "balance_ratio = 0.5\n",
    "random_state = 1\n",
    "sim_type = 'cosine'\n",
    "embedding_method = 'stsb-roberta-large' # try different embeddings and find proper one\n",
    "\n",
    "np.random.seed(random_state)\n",
    "\n",
    "majority_path = r'C:\\Users\\IsmailKaraman\\workspace\\data\\privacy_policy_data\\OPP-115_v2\\majority.csv'\n",
    "\n",
    "all_columns = ['Data Retention', 'Data Security', 'Do Not Track', 'First Party Collection/Use', \n",
    "             'International and Specific Audiences', 'Introductory/Generic', 'Policy Change', \n",
    "             'Practice not covered', 'Privacy contact information', 'Third Party Sharing/Collection',\n",
    "             'User Access, Edit and Deletion', 'User Choice/Control']\n",
    "\n",
    "sub_col_names = ['Data Security', 'User Access, Edit and Deletion', 'Policy Change']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framed-george",
   "metadata": {},
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4d6475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading data\n",
    "df = utilities.read_data(majority_path)\n",
    "# -----------------------------------------------------------------------------------------------------------------------------\n",
    "# creating a toy dataset to test method\n",
    "'''\n",
    "np.random.seed(random_state)\n",
    "toy_df = df[(df[all_columns].sum(axis=1)==df[sub_col_names].sum(axis=1))].sample(100, random_state=random_state)\n",
    "X = toy_df['text']\n",
    "y = toy_df[sub_col_names]\n",
    "all_columns = sub_col_names # note: only for toy example\n",
    "'''\n",
    "X = df['text']\n",
    "y = df[all_columns]\n",
    "# ------------------------------------------------------------------------------------------------------------------------------\n",
    "# reading from a pickle instead of applying vectorization\n",
    "X_num = utilities.vectorize_data(X, embedding_method)\n",
    "X_num = pd.Series([np.squeeze(i) for i in X_num])\n",
    "'''\n",
    "import pickle\n",
    "with open('X_num.p', 'wb') as f:\n",
    "    pickle.dump(X_num, f)     \n",
    "\n",
    "with open('X_num.p', 'rb') as f:\n",
    "    X_num = pickle.load(f)\n",
    "\n",
    "assert np.array_equal(X_num.index, X.index), 'read indexes doesn\\'t match!'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3fbb07",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(271,) (271, 12) (2448,) (2448, 12)\n",
      "\u001b[1mLinearSVM results: \u001b[0m\n",
      "------------------------------\n",
      "hamLoss: 0.07\n",
      "Exact Match Ratio: 0.47\n",
      "------------------------------\n",
      "\u001b[1mClassification Report\u001b[0m\n",
      "                                      precision    recall  f1-score   support\n",
      "\n",
      "                      Data Retention       0.36      0.31      0.33        13\n",
      "                       Data Security       0.76      0.78      0.77        40\n",
      "                        Do Not Track       1.00      0.50      0.67         6\n",
      "          First Party Collection/Use       0.70      0.71      0.71       230\n",
      "International and Specific Audiences       0.81      0.69      0.75        68\n",
      "                Introductory/Generic       0.60      0.46      0.52        76\n",
      "                       Policy Change       0.80      0.91      0.85        22\n",
      "                Practice not covered       0.14      0.08      0.11        24\n",
      "         Privacy contact information       0.68      0.42      0.52        40\n",
      "      Third Party Sharing/Collection       0.70      0.57      0.63       183\n",
      "      User Access, Edit and Deletion       0.73      0.29      0.41        28\n",
      "                 User Choice/Control       0.60      0.56      0.58        66\n",
      "\n",
      "                           micro avg       0.68      0.59      0.63       796\n",
      "                           macro avg       0.66      0.52      0.57       796\n",
      "                        weighted avg       0.68      0.59      0.63       796\n",
      "                         samples avg       0.61      0.63      0.60       796\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------------------------------------------------------\n",
    "# splitting train(labeled-unlabeled)-test\n",
    "# X_num = X.apply(vectorize_data) \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_num, y, test_size=0.2, random_state=random_state)\n",
    "X_labeled, X_unlabeled, y_labeled, y_unlabeled = train_test_split(X_train, y_train, test_size=0.9, random_state=random_state)\n",
    "# -----------------------------------------------------------------------------------------------------------------------------\n",
    "# an initial classifier to see results before applying our method\n",
    "print(X_labeled.shape, y_labeled.shape, X_unlabeled.shape, y_unlabeled.shape)\n",
    "utilities.classifier(np.vstack(X_labeled), y_labeled, np.vstack(X_test), y_test)\n",
    "#utilities.classifier(np.vstack(X_labeled.values), y_labeled, np.vstack(X_test.values), y_test)\n",
    "# -----------------------------------------------------------------------------------------------------------------------------\n",
    "# calculation number of instances to balance dataset\n",
    "balance_ratio = 0.5\n",
    "num_of_new_instances = utilities.calculate_balancing_num_instance_multiclass(y_labeled, balance_ratio)\n",
    "# -----------------------------------------------------------------------------------------------------------------------------\n",
    "# oversampling dataset using unlabeled data with the given ratios\n",
    "validation, X_labeled, y_labeled, X_unlabeled, y_unlabeled = utilities.oversample_dataset(num_of_new_instances, \n",
    "                                                                    X_labeled, y_labeled, X_unlabeled, y_unlabeled)\n",
    "# -----------------------------------------------------------------------------------------------------------------------------\n",
    "# check if the result gets better\n",
    "print(X_labeled.shape, y_labeled.shape, X_unlabeled.shape, y_unlabeled.shape)\n",
    "utilities.classifier(np.vstack(X_labeled.values), y_labeled, np.vstack(X_test.values), y_test)\n",
    "# -----------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ad7a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_res = []\n",
    "for _, value in validation.items():\n",
    "    col, _, _, y_true, y_pred = value\n",
    "    compare_res.append((list(y_true.values), list(y_pred.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92eab65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "emr = 0\n",
    "accuracy = 0\n",
    "for t, p in compare_res:\n",
    "    if t==p:\n",
    "        emr += 1\n",
    "    if t[0] == p[0]:\n",
    "        accuracy +=1\n",
    "    if t[1] == p[1]:\n",
    "        accuracy +=1\n",
    "    if t[2] == p[2]:\n",
    "        accuracy +=1\n",
    "print('Metrics for the proposed algorithm ')    \n",
    "print(f'Exact match ratio : {emr/len(compare_res):.2f} ')\n",
    "print(f'Accuracy          : {accuracy/(len(compare_res)*3):.2f} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f970f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c613fa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b0d91c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b43b4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffddfbd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313e6f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import words\n",
    "all_words = words.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1758031d",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_5len = [word.lower() for word in all_words if len(word)==5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25a530b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "data_vecs = vectorizer.fit_transform(data_words).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certified-missouri",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for word in words_5len:\n",
    "    if 'h' in word and 'k' in word and 'n' in word:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-leisure",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "common_dictionary = Dictionary(data_words)\n",
    "common_corpus = [common_dictionary.doc2bow(text) for text in data_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-gnome",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fallen-klein",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vecs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-niger",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e227fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from scipy.special import kl_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impossible-behalf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for word in words_5len:\n",
    "    if word.startswith('se') and 'r' in word:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colored-glossary",
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_0 = ''\n",
    "letter_1 = ''\n",
    "letter_2 = 'a'\n",
    "letter_3 = ''\n",
    "letter_4 = ''\n",
    "\n",
    "exist_letters = 'acs'\n",
    "banned_letters = 'trdefou'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381168a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL_divergence(p, q):\n",
    "        \"\"\" Compute KL divergence of two vectors, K(p || q).\"\"\"\n",
    "        return sum(p[x] * log((p[x]) / (q[x])) for x in range(len(p)) if p[x] != 0.0 or p[x] != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9933bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import zeros, array\n",
    "from math import sqrt, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ed5861",
   "metadata": {},
   "outputs": [],
   "source": [
    "jensenshannon(p, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlling-memphis",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = [word for word in words_5len for e in exist_letters if e in word]\n",
    "filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b866aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = [word for word in filtered for b in banned_letters if b in word]\n",
    "filtered = [word for word in filtered if letter_0 and word[0]==letter_0]\n",
    "filtered = [word for word in filtered if letter_1 and word[0]==letter_1]\n",
    "filtered = [word for word in filtered if letter_2 and word[0]==letter_2]\n",
    "filtered = [word for word in filtered if letter_3 and word[0]==letter_3]\n",
    "filtered = [word for word in filtered if letter_4 and word[0]==letter_4]\n",
    "filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b69d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JSD(object):\n",
    "    def __init__(self):\n",
    "        self.log2 = log(2)\n",
    "\n",
    "\n",
    "    def KL_divergence(self, p, q):\n",
    "        \"\"\" Compute KL divergence of two vectors, K(p || q).\"\"\"\n",
    "        return sum(p[x] * log((p[x]) / (q[x])) for x in range(len(p)) if p[x] != 0.0 or p[x] != 0)\n",
    "\n",
    "    def Jensen_Shannon_divergence(self, p, q):\n",
    "        \"\"\" Returns the Jensen-Shannon divergence. \"\"\"\n",
    "        self.JSD = 0.0\n",
    "        weight = 0.5\n",
    "        average = zeros(len(p)) #Average\n",
    "        for x in range(len(p)):\n",
    "            average[x] = weight * p[x] + (1 - weight) * q[x]\n",
    "            self.JSD = (weight * self.KL_divergence(array(p), average)) + ((1 - weight) * self.KL_divergence(array(q), average))\n",
    "        return 1-(self.JSD/sqrt(2 * self.log2))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    J = JSD()\n",
    "    p = [1.0/10, 9.0/10, 0]\n",
    "    q = [0, 1.0/10, 9.0/10]\n",
    "    p = data_vecs[0]\n",
    "    q = data_vecs[1]\n",
    "    print(J.Jensen_Shannon_divergence(p, q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spatial-budget",
   "metadata": {},
   "outputs": [],
   "source": [
    "if letter_2:\n",
    "    print('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1ebbe7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for word in words_5len:\n",
    "    if 'o' in word and 'u' in word and 'a' not in word and 'i' not in word and 'd' not in word:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b7269e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
