{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a701e748",
   "metadata": {},
   "source": [
    "TO DO\n",
    "\n",
    "general implementation\n",
    "    create a graph or diagram to tell whats hapenning\n",
    "    mark all the tasks/steps complete, incomplete, in progress, problems, to do, research etc.\n",
    "    try on a toy problem\n",
    "\n",
    "find new datasets\n",
    "    for different datasets different preprocessing techniques should be applied\n",
    "    RCV1-V2\n",
    "decide on splitting ratio 20 60 20 \n",
    "\n",
    "try different similarity measures \n",
    "    reference paper\n",
    "    cosine\n",
    "    euclidean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b372be",
   "metadata": {},
   "source": [
    "implementation steps\n",
    "\n",
    "+1. reading data and preprocessing\n",
    "2. vectorization\n",
    "    -2.1 embeddings - will try other embeddings, and will search which one is best for datasets\n",
    "    -2.2 dimensionality reduction? (is similarity more accurate when dim. red. done)  - research\n",
    "3. initial classifier to show results\n",
    "4. calculate imbalance ratio and find the ratio of newly labeled data\n",
    "5. oversample dataset using unlabeled set\n",
    "    5.1 find the proper similarity function (eclidean, cosine etc.)\n",
    "        Measurement of Text Similarity: A Survey: a very detailed survey of similarity functions that are used for text data\n",
    "        https://www.kdnuggets.com/2019/01/comparison-text-distance-metrics.html\n",
    "        cosine similarity\n",
    "        minkowski family (euclidean, manhattan)\n",
    "        hamming distance\n",
    "        Jaccard index\n",
    "        Sorensen-dice index\n",
    "        KL divergence\n",
    "        Jensen–Shannon divergence with LDA\n",
    "        Wasserstein distance\n",
    "        SMTP \n",
    "        word mover’s distance\n",
    "    5.2 define a threshold or mechanism to add data for multilabeled set\n",
    "6. train a final classifier to compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "important-accent",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import preprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import LinearSVC\n",
    "import torch\n",
    "from itertools import combinations\n",
    "from sentence_transformers import util\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import hamming_loss, accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decimal-packing",
   "metadata": {},
   "source": [
    "## parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "hundred-reading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# algorithm parameters\n",
    "balance_ratio = 0.5\n",
    "sim_type = 'cosine'\n",
    "embedding_method = '' # try different embeddings and find proper one\n",
    "\n",
    "random_state = 1\n",
    "starting_index = 100_000\n",
    "np.random.seed(random_state)\n",
    "\n",
    "majority_path = r'C:\\Users\\IsmailKaraman\\workspace\\data\\privacy_policy_data\\OPP-115_v2\\majority.csv'\n",
    "\n",
    "all_columns = ['Data Retention', 'Data Security', 'Do Not Track', 'First Party Collection/Use', \n",
    "             'International and Specific Audiences', 'Introductory/Generic', 'Policy Change', \n",
    "             'Practice not covered', 'Privacy contact information', 'Third Party Sharing/Collection',\n",
    "             'User Access, Edit and Deletion', 'User Choice/Control']\n",
    "\n",
    "sub_col_names = ['Data Security', 'User Access, Edit and Deletion', 'Policy Change']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5df9753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    df = pd.read_csv(path)\n",
    "    df['text'] = df['text'].apply(preprocess.preprocess_text)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61d22048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_data(text, model_name='stsb-roberta-large'):\n",
    "    \n",
    "    from sentence_transformers import util\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    import torch\n",
    "    \n",
    "    model = SentenceTransformer(model_name)\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    vectors = model.encode(text, convert_to_tensor=False, device=device)\n",
    "    \n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bca5c4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    def calculating_class_weights(y_true):\n",
    "        \n",
    "        number_dim = np.shape(y_true)[1]\n",
    "        weights = []\n",
    "        for i in range(number_dim):\n",
    "            at = compute_class_weight('balanced', [0.,1.], y_true[:, i])\n",
    "            weights.append(dict(zip([0,1], at)))\n",
    "            # weights[i] = compute_class_weight('balanced', [0.,1.], y_true[:, i])))\n",
    "        return weights\n",
    "\n",
    "    # class_weights = calculating_class_weights(y_train.values)\n",
    "    \n",
    "    # Linear SVM\n",
    "    linearSvm = OneVsRestClassifier(LogisticRegression(class_weight='balanced'), n_jobs=-1)\n",
    "    linearSvm.fit(X_train, y_train.values)\n",
    "    linearSvm_preds = linearSvm.predict(X_test)\n",
    "    \n",
    "    print(\"\\033[1m\" + 'LinearSVM results: ' + \"\\033[0m\")\n",
    "    print('-'*30)\n",
    "    hamLoss = hamming_loss(y_test.values, linearSvm_preds)\n",
    "    print('hamLoss: {:.2f}'.format(hamLoss))\n",
    "    acc_score = accuracy_score(y_test.values, linearSvm_preds)\n",
    "    print('Exact Match Ratio: {:.2f}'.format(acc_score))\n",
    "    print('-'*30)\n",
    "    print(\"\\033[1m\" + 'Classification Report' + \"\\033[0m\")\n",
    "    print(classification_report(y_test.values, linearSvm_preds, target_names=list(y_test.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "encouraging-congress",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_imb_ratio(y):\n",
    "\n",
    "    class_ratios = (y.sum() / y.shape[0]).values\n",
    "    return class_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fundamental-planning",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_balancing_num_instance_binary(n_samples, n_total_samples, balance_ratio=0.5):\n",
    "    \n",
    "    if n_samples/n_total_samples > balance_ratio:\n",
    "        print(\"Be careful! Given balancing ratio is lower than the class' imbalance ratio\")\n",
    "        \n",
    "    return int((n_total_samples*balance_ratio - n_samples)*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "suspended-field",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_balancing_num_instance_multiclass(y, balance_ratio):\n",
    "    \n",
    "    oversampling_counts = {}\n",
    "    n_samples = y.shape[0]\n",
    "    n_classes = y.shape[1]\n",
    "    \n",
    "    for col in y.columns:\n",
    "        oversampling_counts[col] = cal_balancing_num_instance_binary(y[col].sum(), n_samples, balance_ratio)\n",
    "    \n",
    "    return oversampling_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9bb30db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "    if norm1 == 0:\n",
    "        norm1 += 0.00001\n",
    "    if norm2 == 0:\n",
    "        norm2 += 0.00001   \n",
    "    return np.dot(vec1, vec2)/(norm1*norm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c0410f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minkowski_similarity(u, v, p=2):\n",
    "    # minkowski distance is a distance measure but we need a similarity function\n",
    "    if p <= 0:\n",
    "        raise ValueError(\"p must be greater than 0\")\n",
    "    u_v = u - v\n",
    "    dist = np.linalg.norm(u_v, ord=p)\n",
    "    if dist == 0:\n",
    "        dist += 0.0001\n",
    "        \n",
    "    return 1/dist #converting a distance to similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d445edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_similarity(vec1, vec2, sim_type=sim_type):\n",
    "    \n",
    "    if sim_type == 'cosine':\n",
    "        similarity = cosine_similarity(vec1, vec2)\n",
    "    if sim_type == 'euclidean':\n",
    "        similarity = minkowski_similarity(vec1, vec2, 2)\n",
    "    if sim_type == 'manhattan':\n",
    "        similarity = minkowski_similarity(vec1, vec2, 1)\n",
    "    if sim_type == 'chebychev ':\n",
    "        similarity = minkowski_similarity(vec1, vec2, np.inf)\n",
    "    if sim_type.startswith('minkowski'):\n",
    "        similarity = minkowski_similarity(vec1, vec2, int(sim_type[-1]))\n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "564e8408",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_within_class_similarity(vecs, sim_type=sim_type):\n",
    "    \n",
    "    similarities = []\n",
    "    \n",
    "    for i,j in list(combinations(vecs.index, 2)):\n",
    "        similarities.append(vector_similarity(vecs.loc[i], vecs.loc[j], sim_type))    \n",
    "            \n",
    "    try:\n",
    "        avg_similarity = sum(similarities)/len(similarities)\n",
    "    except AssertionErrors:\n",
    "        print('Error occured')\n",
    "        \n",
    "    return avg_similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91f0de03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity_between_vector_and_class(vec, class_vecs, sim_type=sim_type):\n",
    "    \n",
    "    similarities = []\n",
    "    \n",
    "    for c_vec in class_vecs:\n",
    "        similarities.append(vector_similarity(vec, c_vec, sim_type))\n",
    "    \n",
    "    try:\n",
    "        avg_similarity = sum(similarities)/len(similarities)\n",
    "    except AssertionErrors:\n",
    "        print('Error occured')\n",
    "        \n",
    "    return avg_similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9b50d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_new_instances(X_labeled, X_unlabeled, class_similarity):\n",
    "    \n",
    "    new_instances = []\n",
    "    \n",
    "    for idx, instance in X_unlabeled.iteritems():\n",
    "        avg_sim = calculate_similarity_between_vector_and_class(instance, X_labeled)\n",
    "        if avg_sim > class_similarity:\n",
    "            new_instances.append(idx)\n",
    "            \n",
    "    return new_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec4f90cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_overall_class_similarities(X, y):\n",
    "    \n",
    "    class_similarities = {}\n",
    "    for col in y.columns:\n",
    "        indexes = (y[col] == 1).index\n",
    "        aa = X.loc[indexes]\n",
    "        class_similarities[col] = calculate_within_class_similarity(aa) \n",
    "        \n",
    "    return class_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "704564e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_columns(instance, X_labeled, y_labeled, other_columns):\n",
    "    \n",
    "    other_similarities = {}\n",
    "    \n",
    "    for col_name in other_columns:\n",
    "        \n",
    "        indexes = (y_labeled[col_name] == 1).index\n",
    "        \n",
    "        other_similarities[col_name]  = calculate_similarity_between_vector_and_class(instance, X_labeled.loc[indexes])\n",
    "    \n",
    "    return other_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "legal-barrier",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversample_dataset(num_of_new_instances, X_labeled, y_labeled, X_unlabeled, y_unlabeled):\n",
    "    \n",
    "    # giving priority to mostly imbalanced classes\n",
    "    num_of_new_instances = {k: v for k, v in sorted(num_of_new_instances.items(), key=lambda item: item[1], reverse=True)}\n",
    "    \n",
    "    class_similarities = calculate_overall_class_similarities(X_labeled, y_labeled)\n",
    "    \n",
    "    processed_columns = []\n",
    "    \n",
    "    validation = {}\n",
    "    val_idx = 0\n",
    "    \n",
    "    for col_name, num_instance in num_of_new_instances.items():\n",
    "        \n",
    "        # note: we didnt use num_instance\n",
    "        # the instances will be added should not exceed num_instance\n",
    "        \n",
    "        processed_columns.append(col_name)\n",
    "        \n",
    "        if num_instance == 0:\n",
    "            continue\n",
    "        \n",
    "        indexes = (y_labeled[col_name] == 1).index\n",
    "        new_instances = find_new_instances(X_labeled.loc[indexes], X_unlabeled, class_similarities[col_name])\n",
    "        \n",
    "        \n",
    "        for instance_index in new_instances:\n",
    "            \n",
    "            instance_X = X_unlabeled.loc[instance_index]\n",
    "            instance_y = y_unlabeled.loc[instance_index] # note: this is for test case\n",
    "            \n",
    "            # defining all labels as 0s\n",
    "            new_labels = {c:0 for c in all_columns}\n",
    "            # changing col_name's label as 1\n",
    "            new_labels[col_name] = 1\n",
    "            \n",
    "            ### finding other labels\n",
    "            other_columns = [i for i in all_columns if i not in processed_columns]\n",
    "            other_similarities = find_similar_columns(instance_X, X_labeled, y_labeled, other_columns)\n",
    "            for col, sim in other_similarities.items():\n",
    "                if sim > class_similarities[col]:\n",
    "                    new_labels[col] = 1\n",
    "            \n",
    "            ### appending data to unlabeled set and removing it from unlabeled set\n",
    "            # starting index of new instances from a big number\n",
    "            instance_new_index = max(starting_index, max(X_labeled.index)) + 1\n",
    "            instance_X_series = pd.Series([instance_X], index=[instance_new_index])\n",
    "            instance_new_labels =pd.DataFrame(new_labels, index=[instance_new_index])\n",
    "            # adding new instance to labeled set\n",
    "            X_labeled = pd.concat([X_labeled, instance_X_series])\n",
    "            y_labeled = pd.concat([y_labeled, instance_new_labels])\n",
    "            # removing new instance from unlabeled set\n",
    "            X_unlabeled.drop(instance_index, inplace=True)\n",
    "            y_unlabeled.drop(instance_index, inplace=True) # note: this is for test case\n",
    "            \n",
    "            # validation\n",
    "            validation[val_idx] = (col_name, instance_index, instance_X, (instance_y), new_labels)\n",
    "            val_idx += 1\n",
    "    \n",
    "    return validation, X_labeled, y_labeled, X_unlabeled, y_unlabeled "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framed-george",
   "metadata": {},
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d6a9fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mLinearSVM results: \u001b[0m\n",
      "------------------------------\n",
      "hamLoss: 0.03\n",
      "Exact Match Ratio: 0.90\n",
      "------------------------------\n",
      "\u001b[1mClassification Report\u001b[0m\n",
      "                                precision    recall  f1-score   support\n",
      "\n",
      "                 Data Security       1.00      1.00      1.00         9\n",
      "User Access, Edit and Deletion       1.00      0.80      0.89         5\n",
      "                 Policy Change       1.00      0.83      0.91         6\n",
      "\n",
      "                     micro avg       1.00      0.90      0.95        20\n",
      "                     macro avg       1.00      0.88      0.93        20\n",
      "                  weighted avg       1.00      0.90      0.94        20\n",
      "                   samples avg       0.90      0.90      0.90        20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mLinearSVM results: \u001b[0m\n",
      "------------------------------\n",
      "hamLoss: 0.38\n",
      "Exact Match Ratio: 0.30\n",
      "------------------------------\n",
      "\u001b[1mClassification Report\u001b[0m\n",
      "                                precision    recall  f1-score   support\n",
      "\n",
      "                 Data Security       0.60      1.00      0.75         9\n",
      "User Access, Edit and Deletion       0.38      1.00      0.56         5\n",
      "                 Policy Change       0.40      1.00      0.57         6\n",
      "\n",
      "                     micro avg       0.47      1.00      0.63        20\n",
      "                     macro avg       0.46      1.00      0.63        20\n",
      "                  weighted avg       0.49      1.00      0.65        20\n",
      "                   samples avg       0.57      1.00      0.69        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reading data\n",
    "df = pd.read_csv(majority_path)\n",
    "# -----------------------------------------------------------------------------------------------------------------------------\n",
    "# creating a toy dataset to provide efficiency\n",
    "np.random.seed(random_state)\n",
    "toy_df = df[(df[all_columns].sum(axis=1)==df[sub_col_names].sum(axis=1))].sample(100, random_state=random_state)\n",
    "# ------------------------------------------------------------------------------------------------------------------------------\n",
    "X = toy_df['text']\n",
    "y = toy_df[sub_col_names]\n",
    "all_columns = sub_col_names # note: only for toy example\n",
    "# ------------------------------------------------------------------------------------------------------------------------------\n",
    "# reading from a pickle instead of applying vectorization\n",
    "'''\n",
    "X_num = X.apply(vectorize_data)\n",
    "import pickle\n",
    "with open('X_num.p', 'wb') as f:\n",
    "    pickle.dump(X_num, f)     \n",
    "'''\n",
    "with open('X_num.p', 'rb') as f:\n",
    "    X_num = pickle.load(f)\n",
    "\n",
    "assert np.array_equal(X_num.index, X.index), 'read indexes doesn\\'t match!'\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------------------------------\n",
    "# splitting train(labeled-unlabeled)-test\n",
    "# X_num = X.apply(vectorize_data) \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_num, y, test_size=0.2, random_state=random_state, stratify=y)\n",
    "X_labeled, X_unlabeled, y_labeled, y_unlabeled = train_test_split(X_train, y_train, test_size=0.9, \n",
    "                                                                  stratify=y_train, random_state=random_state)\n",
    "# -----------------------------------------------------------------------------------------------------------------------------\n",
    "# an initial classifier to see results before applying our method\n",
    "classifier(np.vstack(X_labeled.values), y_labeled, np.vstack(X_test.values), y_test)\n",
    "# -----------------------------------------------------------------------------------------------------------------------------\n",
    "# calculation number of instances to balance dataset\n",
    "balance_ratio = 0.5\n",
    "num_of_new_instances = cal_balancing_num_instance_multiclass(y_labeled, balance_ratio)\n",
    "# -----------------------------------------------------------------------------------------------------------------------------\n",
    "# oversampling dataset using unlabeled data with the given ratios\n",
    "validation, X_labeled, y_labeled, X_unlabeled, y_unlabeled = oversample_dataset(num_of_new_instances, \n",
    "                                                                    X_labeled, y_labeled, X_unlabeled, y_unlabeled)\n",
    "# -----------------------------------------------------------------------------------------------------------------------------\n",
    "# check if the result gets better\n",
    "classifier(np.vstack(X_labeled.values), y_labeled, np.vstack(X_test.values), y_test)\n",
    "# -----------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566b277e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "627d62d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ISMAIL~1\\AppData\\Local\\Temp/ipykernel_1068/3957423419.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62a86f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_res = []\n",
    "for _, value in validation.items():\n",
    "    col, _, _, y_true, y_pred = value\n",
    "    compare_res.append((list(y_true.values), list(y_pred.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d13aaec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hamm_loss = 0\n",
    "accuracy = 0\n",
    "for t, p in compare_res:\n",
    "    if t==p:\n",
    "        hamm_loss += 1\n",
    "    if t[0] == p[0]:\n",
    "        accuracy +=1\n",
    "    if t[1] == p[1]:\n",
    "        accuracy +=1\n",
    "    if t[2] == p[2]:\n",
    "        accuracy +=1\n",
    "print('Metrics for the proposed algorithm ')    \n",
    "print(f'Hamming loss: {hamm_loss/len(compare_res):.2f} ')\n",
    "print(f'Accuracy:     {accuracy/(len(compare_res)*3):.2f} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2d0eaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9acd96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313e6f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import words\n",
    "all_words = words.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1758031d",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_5len = [word.lower() for word in all_words if len(word)==5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b27147",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "data_vecs = vectorizer.fit_transform(data_words).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certified-missouri",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for word in words_5len:\n",
    "    if 'h' in word and 'k' in word and 'n' in word:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-leisure",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "common_dictionary = Dictionary(data_words)\n",
    "common_corpus = [common_dictionary.doc2bow(text) for text in data_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-gnome",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fallen-klein",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vecs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-niger",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d18885",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from scipy.special import kl_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impossible-behalf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for word in words_5len:\n",
    "    if word.startswith('se') and 'r' in word:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colored-glossary",
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_0 = ''\n",
    "letter_1 = ''\n",
    "letter_2 = 'a'\n",
    "letter_3 = ''\n",
    "letter_4 = ''\n",
    "\n",
    "exist_letters = 'acs'\n",
    "banned_letters = 'trdefou'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6efc54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL_divergence(p, q):\n",
    "        \"\"\" Compute KL divergence of two vectors, K(p || q).\"\"\"\n",
    "        return sum(p[x] * log((p[x]) / (q[x])) for x in range(len(p)) if p[x] != 0.0 or p[x] != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafb077a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import zeros, array\n",
    "from math import sqrt, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b59f06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "jensenshannon(p, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlling-memphis",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = [word for word in words_5len for e in exist_letters if e in word]\n",
    "filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b866aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = [word for word in filtered for b in banned_letters if b in word]\n",
    "filtered = [word for word in filtered if letter_0 and word[0]==letter_0]\n",
    "filtered = [word for word in filtered if letter_1 and word[0]==letter_1]\n",
    "filtered = [word for word in filtered if letter_2 and word[0]==letter_2]\n",
    "filtered = [word for word in filtered if letter_3 and word[0]==letter_3]\n",
    "filtered = [word for word in filtered if letter_4 and word[0]==letter_4]\n",
    "filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cec711",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JSD(object):\n",
    "    def __init__(self):\n",
    "        self.log2 = log(2)\n",
    "\n",
    "\n",
    "    def KL_divergence(self, p, q):\n",
    "        \"\"\" Compute KL divergence of two vectors, K(p || q).\"\"\"\n",
    "        return sum(p[x] * log((p[x]) / (q[x])) for x in range(len(p)) if p[x] != 0.0 or p[x] != 0)\n",
    "\n",
    "    def Jensen_Shannon_divergence(self, p, q):\n",
    "        \"\"\" Returns the Jensen-Shannon divergence. \"\"\"\n",
    "        self.JSD = 0.0\n",
    "        weight = 0.5\n",
    "        average = zeros(len(p)) #Average\n",
    "        for x in range(len(p)):\n",
    "            average[x] = weight * p[x] + (1 - weight) * q[x]\n",
    "            self.JSD = (weight * self.KL_divergence(array(p), average)) + ((1 - weight) * self.KL_divergence(array(q), average))\n",
    "        return 1-(self.JSD/sqrt(2 * self.log2))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    J = JSD()\n",
    "    p = [1.0/10, 9.0/10, 0]\n",
    "    q = [0, 1.0/10, 9.0/10]\n",
    "    p = data_vecs[0]\n",
    "    q = data_vecs[1]\n",
    "    print(J.Jensen_Shannon_divergence(p, q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spatial-budget",
   "metadata": {},
   "outputs": [],
   "source": [
    "if letter_2:\n",
    "    print('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1ebbe7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for word in words_5len:\n",
    "    if 'o' in word and 'u' in word and 'a' not in word and 'i' not in word and 'd' not in word:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b7269e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
