{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8edd5e50",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import utilities\n",
    "import preprocess\n",
    "import parameters\n",
    "\n",
    "from sklearn.metrics import accuracy_score, hamming_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sentence_transformers import util, SentenceTransformer\n",
    "from sklearn.metrics import hamming_loss, accuracy_score, f1_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "095e9601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decimal-packing",
   "metadata": {},
   "source": [
    "## parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f677899c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing algorithm parameters\n",
    "sim_type = parameters.sim_type\n",
    "random_state = 2\n",
    "test_size = parameters.test_size\n",
    "\n",
    "# doe\n",
    "balance_ratio = parameters.balance_ratio\n",
    "sim_calculation_type = parameters.sim_calculation_type\n",
    "\n",
    "success_metric = parameters.success_metric\n",
    "embedding_method = parameters.embedding_method\n",
    "data_paths = parameters.data_paths\n",
    "X_num_paths = parameters.X_num_paths\n",
    "unlabeled_ratios = parameters.unlabeled_ratios\n",
    "\n",
    "np.random.seed(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e71ff4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "hundred-reading",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_object = SVC(kernel='linear',probability=True, class_weight='balanced')\n",
    "# classifier_object = LogisticRegression(class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d2e44ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    X_labeled, X_unlabeled, y_labeled, y_unlabeled = train_test_split(X_train, y_train, test_size=unlabeled_ratios[data], \n",
    "                                                                  random_state=random_state)\n",
    "    \n",
    "    return X_labeled, y_labeled, X_unlabeled, y_unlabeled, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a51e834f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_KFold(X, y, cv):\n",
    "    \n",
    "    from sklearn.model_selection import KFold, StratifiedKFold\n",
    "    \n",
    "    sorted_y = y.sum().sort_values(ascending=False)\n",
    "    for col, _ in zip(sorted_y.index, sorted_y):\n",
    "        y.loc[y[y[col]==1].index, 'dominant_label'] = col\n",
    "        \n",
    "    stratify_flag = y['dominant_label']\n",
    "    y.drop(['dominant_label'], axis=1, inplace=True)\n",
    "    \n",
    "    kf = StratifiedKFold(n_splits=cv, random_state=random_state, shuffle=True)\n",
    "    \n",
    "    splits = []\n",
    "    for train_idx, test_idx in kf.split(X.index, stratify_flag):\n",
    "        \n",
    "        labeled_idx, unlabeled_idx = train_test_split(train_idx, test_size=unlabeled_ratios[data], random_state=random_state)\n",
    "        \n",
    "        splits.append((labeled_idx, unlabeled_idx, test_idx))\n",
    "        \n",
    "    return splits    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d2d996f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data):\n",
    "    # reading data\n",
    "    df = utilities.read_data(data_paths[data])\n",
    "    # X = df['text'].apply(preprocess.preprocess_text)\n",
    "    y = df.drop(['text'], axis=1)\n",
    "    # ------------------------------------------------------------------------------------------------------------------------------\n",
    "    # reading from a pickle instead of applying vectorization\n",
    "    # X_num = utilities.vectorize_data(X, embedding_method)\n",
    "    # X_num = pd.Series([np.squeeze(i) for i in X_num])\n",
    "    X = pd.read_pickle(X_num_paths[data])\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a0b3fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dimension(X, n_comp=200):\n",
    "    \n",
    "    X_ = np.vstack(X.values)\n",
    "\n",
    "    from sklearn.decomposition import TruncatedSVD\n",
    "    svd = TruncatedSVD(n_components=n_comp)\n",
    "    X_reduced = svd.fit_transform(X_)\n",
    "    \n",
    "    return pd.Series([np.squeeze(i) for i in X_reduced], index=X.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framed-george",
   "metadata": {},
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "british-gross",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(data, balance_ratio, sim_calculation_type, single_metric, oversampler_version, batch_size, n_iter, sim_type):\n",
    "    \n",
    "    print('*'*100)\n",
    "    print('\\x1b[1;31m'+data+'\\x1b[0m')\n",
    "    \n",
    "    X, y = read_data(data)\n",
    "    random_state=6\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    X_labeled, X_unlabeled, y_labeled, y_unlabeled = train_test_split(X_train, y_train, test_size=unlabeled_ratios[data], \n",
    "                                                                  random_state=random_state)\n",
    "    print(y_labeled.sum())\n",
    "    shape_before = X_labeled.shape[0]\n",
    "    print(X_labeled.shape, X_unlabeled.shape, X_test.shape)\n",
    "    s_metric = utilities.multilabel_classifier(np.vstack(X_labeled), y_labeled, np.vstack(X_test), y_test, \n",
    "                                               success_metric=success_metric,\n",
    "                                               classifier_object = classifier_object, \n",
    "                                               print_results=True)\n",
    "    # -----------------------------------------------------------------------------------------------------------------------------\n",
    "    # calculation number of instances to balance dataset\n",
    "    num_of_new_instances = utilities.calculate_balancing_num_instance_multiclass(y_labeled, balance_ratio, \n",
    "                                                                                 calculation_type='metric_based', \n",
    "                                                                                 s_metrics=s_metric)\n",
    "    # -----------------------------------------------------------------------------------------------------------------------------\n",
    "    # oversampling dataset using unlabeled data with the given ratios\n",
    "    # print('num_of_new_instances : ',num_of_new_instances)\n",
    "    if oversampler_version == 'v1':\n",
    "        validation, X_labeled, y_labeled, X_unlabeled, y_unlabeled = utilities.oversample_dataset_v1(\n",
    "                                                                        num_of_new_instances, X_labeled, y_labeled, \n",
    "                                                                        X_unlabeled, y_unlabeled, X_test, y_test, \n",
    "                                                                        sim_calculation_type=sim_calculation_type,\n",
    "                                                                        batch_size=batch_size)\n",
    "    elif oversampler_version == 'v2':\n",
    "        validation, X_labeled, y_labeled, X_unlabeled, y_unlabeled = utilities.oversample_dataset_v2(\n",
    "                                                                        num_of_new_instances, X_labeled, y_labeled, \n",
    "                                                                        X_unlabeled, y_unlabeled, X_test, y_test, \n",
    "                                                                        sim_calculation_type=sim_calculation_type,\n",
    "                                                                        batch_size=batch_size)\n",
    "    elif oversampler_version == 'v3':\n",
    "        validation, X_labeled, y_labeled, X_unlabeled, y_unlabeled = utilities.oversample_dataset_v3(\n",
    "                                                                        num_of_new_instances, X_labeled, y_labeled, \n",
    "                                                                        X_unlabeled, y_unlabeled, X_test, y_test, \n",
    "                                                                        sim_calculation_type=sim_calculation_type,\n",
    "                                                                        batch_size=batch_size, \n",
    "                                                                        n_iter=n_iter, \n",
    "                                                                        single_score=single_metric)\n",
    "    elif oversampler_version == 'v4':\n",
    "        validation, X_labeled, y_labeled, X_unlabeled, y_unlabeled, metric_history = utilities.oversample_dataset_v4(\n",
    "                                                                                         num_of_new_instances, \n",
    "                                                                                         X_labeled, y_labeled, \n",
    "                                                                                         X_unlabeled, y_unlabeled, \n",
    "                                                                                         X_test, y_test, \n",
    "                                                                                         sim_calculation_type=sim_calculation_type, \n",
    "                                                                                         batch_size=batch_size, \n",
    "                                                                                         n_iter=n_iter,\n",
    "                                                                                         balance_ratio=balance_ratio,\n",
    "                                                                                         success_metric=success_metric,\n",
    "                                                                                         single_score=single_metric,\n",
    "                                                                                         sim_type=sim_type)\n",
    "    # -----------------------------------------------------------------------------------------------------------------------------\n",
    "    # check if the result gets better\n",
    "    shape_after = X_labeled.shape[0]\n",
    "    s_metric = utilities.multilabel_classifier(np.vstack(X_labeled), y_labeled, np.vstack(X_test), y_test, \n",
    "                                               success_metric=success_metric,\n",
    "                                               classifier_object = classifier_object, \n",
    "                                               print_results=True)\n",
    "    # comparing the found labels and ground truth\n",
    "    y_true, y_pred = [], []\n",
    "    for _, _, _, y_t, y_p in validation:\n",
    "        y_true.append(list(y_t.values))\n",
    "        y_pred.append(list(y_p.values()))\n",
    "    \n",
    "    acc = 1-hamming_loss(y_true, y_pred)\n",
    "    emr = accuracy_score(y_true, y_pred)  \n",
    "    print('-'*30)\n",
    "    print(f'Shape: before {shape_before}, after {shape_after} : {shape_after-shape_before} instances added...')\n",
    "    print(f'Exact match ratio : {emr:.2f} ')\n",
    "    print(f'Accuracy          : {acc:.2f} ')\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print('-'*30)\n",
    "    \n",
    "    if oversampler_version == 'v4':\n",
    "        return metric_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4078cc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_CV(data, balance_ratio, sim_calculation_type, single_metric, oversampler_version, batch_size, n_iter, sim_type):\n",
    "    \n",
    "    CV_results = []\n",
    "    \n",
    "    X, y = read_data(data)\n",
    "    X = reduce_dimension(X)        \n",
    "\n",
    "    splits = split_data_KFold(X, y, 5)\n",
    "\n",
    "    for labeled_idx, unlabeled_idx, test_idx in splits:\n",
    "\n",
    "        X_labeled = X.loc[labeled_idx]\n",
    "        y_labeled = y.loc[labeled_idx]\n",
    "        X_unlabeled = X.loc[unlabeled_idx]\n",
    "        y_unlabeled = y.loc[unlabeled_idx]\n",
    "        X_test = X.loc[test_idx]\n",
    "        y_test = y.loc[test_idx]\n",
    "    \n",
    "    \n",
    "        shape_before = X_labeled.shape[0]\n",
    "\n",
    "        s_metric_before, initial_scores = utilities.multilabel_classifier(np.vstack(X_labeled), y_labeled, np.vstack(X_test), y_test, \n",
    "                                                   success_metric=success_metric,\n",
    "                                                   classifier_object = classifier_object, \n",
    "                                                   print_results=False, return_scores=True)\n",
    "        # -----------------------------------------------------------------------------------------------------------------------------\n",
    "        # calculation number of instances to balance dataset\n",
    "        num_of_new_instances = utilities.calculate_balancing_num_instance_multiclass(y_labeled, balance_ratio, \n",
    "                                                                                     calculation_type='metric_based', \n",
    "                                                                                     s_metrics=s_metric_before)\n",
    "        # -----------------------------------------------------------------------------------------------------------------------------\n",
    "        # oversampling dataset using unlabeled data with the given ratios\n",
    "        # print('num_of_new_instances : ',num_of_new_instances)\n",
    "        if oversampler_version == 'v1':\n",
    "            validation, X_labeled, y_labeled, X_unlabeled, y_unlabeled = utilities.oversample_dataset_v1(\n",
    "                                                                            num_of_new_instances, X_labeled, y_labeled, \n",
    "                                                                            X_unlabeled, y_unlabeled, X_test, y_test, \n",
    "                                                                            sim_calculation_type=sim_calculation_type,\n",
    "                                                                            batch_size=batch_size)\n",
    "        elif oversampler_version == 'v2':\n",
    "            validation, X_labeled, y_labeled, X_unlabeled, y_unlabeled = utilities.oversample_dataset_v2(\n",
    "                                                                            num_of_new_instances, X_labeled, y_labeled, \n",
    "                                                                            X_unlabeled, y_unlabeled, X_test, y_test, \n",
    "                                                                            sim_calculation_type=sim_calculation_type,\n",
    "                                                                            batch_size=batch_size)\n",
    "        elif oversampler_version == 'v3':\n",
    "            validation, X_labeled, y_labeled, X_unlabeled, y_unlabeled = utilities.oversample_dataset_v3(\n",
    "                                                                            num_of_new_instances, X_labeled, y_labeled, \n",
    "                                                                            X_unlabeled, y_unlabeled, X_test, y_test, \n",
    "                                                                            sim_calculation_type=sim_calculation_type,\n",
    "                                                                            batch_size=batch_size, \n",
    "                                                                            n_iter=n_iter, \n",
    "                                                                            single_score=single_metric)\n",
    "        elif oversampler_version == 'v4':\n",
    "            validation, X_labeled, y_labeled, X_unlabeled, y_unlabeled, metric_history = utilities.oversample_dataset_v4(\n",
    "                                                                                             num_of_new_instances, \n",
    "                                                                                             X_labeled, y_labeled, \n",
    "                                                                                             X_unlabeled, y_unlabeled, \n",
    "                                                                                             X_test, y_test, \n",
    "                                                                                             sim_calculation_type=sim_calculation_type, \n",
    "                                                                                             batch_size=batch_size, \n",
    "                                                                                             n_iter=n_iter,\n",
    "                                                                                             balance_ratio=balance_ratio,\n",
    "                                                                                             success_metric=success_metric,\n",
    "                                                                                             single_score=single_metric,\n",
    "                                                                                             sim_type=sim_type)\n",
    "        # -----------------------------------------------------------------------------------------------------------------------------\n",
    "        # check if the result gets better\n",
    "        shape_after = X_labeled.shape[0]\n",
    "        s_metric_after, final_scores = utilities.multilabel_classifier(np.vstack(X_labeled), y_labeled, np.vstack(X_test), y_test, \n",
    "                                                   success_metric=success_metric,\n",
    "                                                   classifier_object = classifier_object, \n",
    "                                                   print_results=False, return_scores=True)\n",
    "        # comparing the found labels and ground truth\n",
    "        y_true, y_pred = [], []\n",
    "        for _, _, _, y_t, y_p in validation:\n",
    "            y_true.append(list(y_t.values))\n",
    "            y_pred.append(list(y_p.values()))\n",
    "        \n",
    "        acc = 1-hamming_loss(y_true, y_pred)\n",
    "        emr = accuracy_score(y_true, y_pred)  \n",
    "        '''\n",
    "        print('-'*30)\n",
    "        print(f'Shape: before {shape_before}, after {shape_after} : {shape_after-shape_before} instances added...')\n",
    "        print(f'Exact match ratio : {emr:.2f} ')\n",
    "        print(f'Accuracy          : {acc:.2f} ')\n",
    "        print(classification_report(y_true, y_pred))\n",
    "        print('-'*30)\n",
    "        '''\n",
    "        if y_true and y_pred:\n",
    "            clf_report = classification_report(y_true, y_pred)\n",
    "        else:\n",
    "            clf_report = ''\n",
    "            \n",
    "        CV_results.append({'shape_before':shape_before,'shape_after':shape_after, 'val_accuracy':acc, 'val_exact_match':emr, \n",
    "                          's_metric_before':s_metric_before, 's_metric_after':s_metric_after, 'initial_scores':initial_scores,\n",
    "                          'final_scores':final_scores, 'validation':validation, 'clf_report':clf_report})\n",
    "        \n",
    "    return CV_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07e5f54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_CV(data, balance_ratio, sim_calculation_type, single_metric, oversampler_version, batch_size, n_iter, sim_type):\n",
    "    \n",
    "    CV_results = run_CV(data, balance_ratio, sim_calculation_type, single_metric, oversampler_version, batch_size, n_iter, sim_type)\n",
    "    \n",
    "    return CV_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a69c7c",
   "metadata": {},
   "source": [
    "data = 'opp115'\n",
    "balance_ratio = 0.5\n",
    "sim_calculation_type = 'average'\n",
    "single_metric = 'f1_score'\n",
    "batch_size = 1\n",
    "n_iter = 200\n",
    "sim_type = 'cosine'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f5102a",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "CV_res = main_CV(data, balance_ratio, sim_calculation_type, single_metric, 'v4', batch_size, n_iter, sim_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8da445",
   "metadata": {},
   "source": [
    "for i in CV_res:\n",
    "    print(i['initial_scores']['f1_score']) \n",
    "    print(i['final_scores']['f1_score'])\n",
    "    print(i['shape_before'], i['shape_after'])\n",
    "    print('-----')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d807731",
   "metadata": {},
   "source": [
    "### experimentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2da2faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {}\n",
    "parameters['data'] = ['opp115', 'ohsumed', 'reuters']\n",
    "parameters['balance_ratio'] = [0.2, 0.5]\n",
    "parameters['sim_calculation_type'] = ['average', 'safe_interval']\n",
    "parameters['batch_size'] = [1, 3, 5]\n",
    "parameters['n_iter'] = [100, 500]\n",
    "parameters['sim_type'] = ['cosine', 'euclidean', 'JS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b4840be",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_size = 10\n",
    "data = 'reuters'\n",
    "single_metric = 'f1_score'\n",
    "file_name = 'results_reuters_1.p'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98aaaff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_param_grid = []\n",
    "for balance_ratio in parameters['balance_ratio']:\n",
    "    for sim_calculation_type in parameters['sim_calculation_type']:\n",
    "        for batch_size in parameters['batch_size']:\n",
    "            for n_iter in parameters['n_iter']:\n",
    "                for sim_type in parameters['sim_type']:\n",
    "                    for run_num in range(1,run_size+1):\n",
    "                        param_list = [run_num, balance_ratio, sim_calculation_type, batch_size, n_iter, sim_type, single_metric]\n",
    "                        all_param_grid.append(param_list)\n",
    "\n",
    "results_df = pd.DataFrame(all_param_grid, columns=['run', 'balance_ratio', 'sim_calculation_type', 'batch_size', 'n_iter', 'sim_type','single_metric'])\n",
    "results_df['result'] = '-'\n",
    "results_df.to_pickle(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78752067",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_pickle(file_name)\n",
    "iter_df = results_df[results_df['result']=='-']\n",
    "iter_df = iter_df[iter_df['sim_type']=='cosine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be4a6adb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c3dbaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx, row in iter_df.iterrows():\n",
    "    \n",
    "    run_num, balance_ratio, sim_calculation_type, batch_size, n_iter, sim_type, single_metric, _ = row\n",
    "    CV_res = main_CV(data, balance_ratio, sim_calculation_type, single_metric, 'v4', batch_size, n_iter, sim_type)\n",
    "    \n",
    "    results_df.loc[idx, 'result'] = str(CV_res)\n",
    "    \n",
    "    results_df.to_pickle(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe9e508",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
