{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "realistic-department",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_flag = 'opp115'\n",
    "\n",
    "# ohsumed: 23986\n",
    "# opp115 3399\n",
    "# reuters 10788"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "important-accent",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import utilities\n",
    "import preprocess\n",
    "from sklearn.metrics import accuracy_score, hamming_loss\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decimal-packing",
   "metadata": {},
   "source": [
    "## parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "hundred-reading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# algorithm parameters\n",
    "balance_ratio = 0.5\n",
    "random_state = 1\n",
    "threshold_factor = 1.5\n",
    "test_size = 0.2\n",
    "\n",
    "sim_type = 'cosine'\n",
    "embedding_method = 'distiluse-base-multilingual-cased-v1' # try different embeddings and find proper one\n",
    "\n",
    "np.random.seed(random_state)\n",
    "\n",
    "data_paths = {'opp115'   : r'C:\\Users\\IsmailKaraman\\workspace\\data\\privacy_policy_data\\OPP-115_v2\\majority.csv',\n",
    "              'ohsumed'  : r'C:\\Users\\IsmailKaraman\\workspace\\GitHub\\thesis\\data\\ohsumed.csv',\n",
    "              'reuters'  : r'C:\\Users\\IsmailKaraman\\workspace\\GitHub\\thesis\\data\\Reuters21578.csv'}\n",
    "\n",
    "unlabaled_ratios = {'opp115':0.75, 'ohsumed':0.95, 'reuters':0.9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "known-chess",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framed-george",
   "metadata": {},
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "british-gross",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(data):\n",
    "    print('*'*100)\n",
    "    print('\\x1b[1;31m'+data+'\\x1b[0m')\n",
    "    # reading data\n",
    "    df = utilities.read_data(data_paths[data])\n",
    "    X = df['text'].apply(preprocess.preprocess_text)\n",
    "    y = df.drop(['text'], axis=1)\n",
    "    # ------------------------------------------------------------------------------------------------------------------------------\n",
    "    # reading from a pickle instead of applying vectorization\n",
    "    X_num = utilities.vectorize_data(X, embedding_method)\n",
    "    X_num = pd.Series([np.squeeze(i) for i in X_num])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_num, y, test_size=test_size, random_state=random_state)\n",
    "    X_labeled, X_unlabeled, y_labeled, y_unlabeled = train_test_split(X_train, y_train, test_size=unlabaled_ratios[data], \n",
    "                                                                  random_state=random_state)\n",
    "    \n",
    "    print(X_labeled.shape, y_labeled.shape, X_unlabeled.shape, y_unlabeled.shape)\n",
    "    utilities.multilabel_classifier(np.vstack(X_labeled), y_labeled, np.vstack(X_test), y_test)\n",
    "    #utilities.classifier(np.vstack(X_labeled.values), y_labeled, np.vstack(X_test.values), y_test)\n",
    "    # -----------------------------------------------------------------------------------------------------------------------------\n",
    "    # calculation number of instances to balance dataset\n",
    "    balance_ratio = 0.5\n",
    "    num_of_new_instances = utilities.calculate_balancing_num_instance_multiclass(y_labeled, balance_ratio)\n",
    "    # -----------------------------------------------------------------------------------------------------------------------------\n",
    "    # oversampling dataset using unlabeled data with the given ratios\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    validation, X_labeled, y_labeled, X_unlabeled, y_unlabeled = utilities.oversample_dataset(num_of_new_instances, \n",
    "                                                                                              X_labeled, y_labeled,\n",
    "                                                                                              X_unlabeled, y_unlabeled, \n",
    "                                                                                              X_test, y_test, \n",
    "                                                                                              sim_calculation_type='safe_interval', \n",
    "                                                                                              batch_size=5)\n",
    "        \n",
    "    '''    \n",
    "    validation, X_labeled, y_labeled, X_unlabeled, y_unlabeled = utilities.oversample_dataset_with_threshold_update(\\\n",
    "                num_of_new_instances, X_labeled, y_labeled, X_unlabeled, y_unlabeled, X_test, y_test, \\\n",
    "                                                    sim_calculation_type='safe_interval', batch_size=5)\n",
    "    # -----------------------------------------------------------------------------------------------------------------------------\n",
    "    # check if the result gets better\n",
    "    print(X_labeled.shape, X_unlabeled.shape, X_test.shape)\n",
    "    utilities.multilabel_classifier(np.vstack(X_labeled.values), y_labeled, np.vstack(X_test.values), y_test)  \n",
    "    \n",
    "    # comparing the found labels and ground truth\n",
    "    y_true, y_pred = [], []\n",
    "    for _, _, _, y_t, y_p in validation:\n",
    "        y_true.append(list(y_t.values))\n",
    "        y_pred.append(list(y_p.values()))\n",
    "    \n",
    "    acc = 1-hamming_loss(y_true, y_pred)\n",
    "    emr = accuracy_score(y_true, y_pred)  \n",
    "    print('-'*30)\n",
    "    print(f'Exact match ratio : {emr:.2f} ')\n",
    "    print(f'Accuracy          : {acc:.2f} ')\n",
    "    print('-'*30)\n",
    "    \n",
    "    print('*'*100)\n",
    "    print('/'*100)\n",
    "    print('*'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "satisfactory-socket",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "\u001b[1;31mopp115\u001b[0m\n",
      "(679,) (679, 12) (2040,) (2040, 12)\n",
      "| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | \n",
      "\u001b[1mMultilabel Classifier Results\u001b[0m\n",
      "\u001b[1mLinearSVM\u001b[0m\n",
      "------------------------------\n",
      "hamLoss: 0.07\n",
      "Exact Match Ratio: 0.46\n",
      "------------------------------\n",
      "\u001b[1mClassification Report\u001b[0m\n",
      "                                      precision    recall  f1-score   support\n",
      "\n",
      "                      Data Retention       0.11      0.23      0.15        13\n",
      "                       Data Security       0.69      0.88      0.77        40\n",
      "                        Do Not Track       0.86      1.00      0.92         6\n",
      "          First Party Collection/Use       0.73      0.88      0.80       230\n",
      "International and Specific Audiences       0.92      0.81      0.86        68\n",
      "                Introductory/Generic       0.50      0.70      0.59        76\n",
      "                       Policy Change       0.74      0.91      0.82        22\n",
      "                Practice not covered       0.18      0.33      0.24        24\n",
      "         Privacy contact information       0.54      0.68      0.60        40\n",
      "      Third Party Sharing/Collection       0.68      0.72      0.70       183\n",
      "      User Access, Edit and Deletion       0.62      0.64      0.63        28\n",
      "                 User Choice/Control       0.57      0.71      0.63        66\n",
      "\n",
      "                           micro avg       0.63      0.76      0.69       796\n",
      "                           macro avg       0.59      0.71      0.64       796\n",
      "                        weighted avg       0.66      0.76      0.70       796\n",
      "                         samples avg       0.68      0.79      0.70       796\n",
      "\n",
      "| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | \n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'similarities' has no attribute 'calculate_similarity_factors'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-2d9c0804a2f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_paths\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-99027d75094f>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     '''    \n\u001b[1;32m---> 36\u001b[1;33m     validation, X_labeled, y_labeled, X_unlabeled, y_unlabeled = utilities.oversample_dataset_with_threshold_update(\\\n\u001b[0m\u001b[0;32m     37\u001b[0m                 \u001b[0mnum_of_new_instances\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_labeled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_labeled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_unlabeled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_unlabeled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m                                                     sim_calculation_type='safe_interval', batch_size=5)\n",
      "\u001b[1;32m~\\workspace\\GitHub\\thesis\\utilities.py\u001b[0m in \u001b[0;36moversample_dataset_with_threshold_update\u001b[1;34m(num_of_new_instances, X_labeled, y_labeled, X_unlabeled, y_unlabeled, X_test, y_test, sim_calculation_type, batch_size)\u001b[0m\n\u001b[0;32m    331\u001b[0m     \u001b[0mclass_similarities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalculate_overall_class_similarities\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_labeled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_labeled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msim_calculation_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 333\u001b[1;33m     \u001b[0msimilarity_factors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalculate_similarity_factors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_similarities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'similarities' has no attribute 'calculate_similarity_factors'"
     ]
    }
   ],
   "source": [
    "for data in data_paths.keys():\n",
    "    main(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f970f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09141a9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d9a2c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea82aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample_dataset_with_threshold_update(num_of_new_instances, X_labeled, y_labeled, X_unlabeled, y_unlabeled, X_test, y_test, sim_calculation_type, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71191275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal(x):\n",
    "    \n",
    "    return x + ((1-x)**2) * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b214e2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.9 + 0.1*0.1*0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d85433",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    x = (i+1)/10\n",
    "    print(i, cal(x)/x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b4bd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347bc82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.1, 0.2, 0.3, 0.4 0.7, 0.9 \n",
    "\n",
    "x*()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff68d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,12,3 ):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c613fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {1:'a', 2:'b', 4:'d', 3:'c'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e98c3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = list(a.keys())\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313e6f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import words\n",
    "all_words = words.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1758031d",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_5len = [word.lower() for word in all_words if len(word)==5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25a530b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "data_vecs = vectorizer.fit_transform(data_words).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certified-missouri",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for word in words_5len:\n",
    "    if 'h' in word and 'k' in word and 'n' in word:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-leisure",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "common_dictionary = Dictionary(data_words)\n",
    "common_corpus = [common_dictionary.doc2bow(text) for text in data_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-gnome",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fallen-klein",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vecs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-niger",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e227fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from scipy.special import kl_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impossible-behalf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for word in words_5len:\n",
    "    if word.startswith('se') and 'r' in word:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colored-glossary",
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_0 = ''\n",
    "letter_1 = ''\n",
    "letter_2 = 'a'\n",
    "letter_3 = ''\n",
    "letter_4 = ''\n",
    "\n",
    "exist_letters = 'acs'\n",
    "banned_letters = 'trdefou'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381168a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL_divergence(p, q):\n",
    "        \"\"\" Compute KL divergence of two vectors, K(p || q).\"\"\"\n",
    "        return sum(p[x] * log((p[x]) / (q[x])) for x in range(len(p)) if p[x] != 0.0 or p[x] != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9933bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import zeros, array\n",
    "from math import sqrt, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ed5861",
   "metadata": {},
   "outputs": [],
   "source": [
    "jensenshannon(p, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlling-memphis",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = [word for word in words_5len for e in exist_letters if e in word]\n",
    "filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b866aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = [word for word in filtered for b in banned_letters if b in word]\n",
    "filtered = [word for word in filtered if letter_0 and word[0]==letter_0]\n",
    "filtered = [word for word in filtered if letter_1 and word[0]==letter_1]\n",
    "filtered = [word for word in filtered if letter_2 and word[0]==letter_2]\n",
    "filtered = [word for word in filtered if letter_3 and word[0]==letter_3]\n",
    "filtered = [word for word in filtered if letter_4 and word[0]==letter_4]\n",
    "filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b69d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JSD(object):\n",
    "    def __init__(self):\n",
    "        self.log2 = log(2)\n",
    "\n",
    "\n",
    "    def KL_divergence(self, p, q):\n",
    "        \"\"\" Compute KL divergence of two vectors, K(p || q).\"\"\"\n",
    "        return sum(p[x] * log((p[x]) / (q[x])) for x in range(len(p)) if p[x] != 0.0 or p[x] != 0)\n",
    "\n",
    "    def Jensen_Shannon_divergence(self, p, q):\n",
    "        \"\"\" Returns the Jensen-Shannon divergence. \"\"\"\n",
    "        self.JSD = 0.0\n",
    "        weight = 0.5\n",
    "        average = zeros(len(p)) #Average\n",
    "        for x in range(len(p)):\n",
    "            average[x] = weight * p[x] + (1 - weight) * q[x]\n",
    "            self.JSD = (weight * self.KL_divergence(array(p), average)) + ((1 - weight) * self.KL_divergence(array(q), average))\n",
    "        return 1-(self.JSD/sqrt(2 * self.log2))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    J = JSD()\n",
    "    p = [1.0/10, 9.0/10, 0]\n",
    "    q = [0, 1.0/10, 9.0/10]\n",
    "    p = data_vecs[0]\n",
    "    q = data_vecs[1]\n",
    "    print(J.Jensen_Shannon_divergence(p, q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spatial-budget",
   "metadata": {},
   "outputs": [],
   "source": [
    "if letter_2:\n",
    "    print('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1ebbe7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for word in words_5len:\n",
    "    if 'o' in word and 'u' in word and 'a' not in word and 'i' not in word and 'd' not in word:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b7269e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9841b3c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
