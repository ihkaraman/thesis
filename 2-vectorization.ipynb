{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import preprocess\n",
    "import utilities\n",
    "import similarities\n",
    "import parameters\n",
    "import seaborn as sns\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths = parameters.data_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find more embeddings\n",
    "https://huggingface.co/nlpaueb/legal-bert-base-uncased\n",
    "http://jalammar.github.io/illustrated-bert/\n",
    "https://www.kdnuggets.com/2021/11/guide-word-embedding-techniques-nlp.html\n",
    "Universal Sentence Encoder + Dense NN\n",
    "Universal Sentence Encoder + ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_BERT_WEIGHTS = download_once_pretrained_transformers(\n",
    "    \"google/bert_uncased_L-4_H-256_A-4\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_BERT_WEIGHTS)\n",
    "model = AutoModel.from_pretrained(PRETRAINED_BERT_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentence_embeddings =  parameters.huggingface_embeddings + parameters.openai_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_between_class_similarities(col1, col2, X, y):\n",
    "    \n",
    "    sims = []\n",
    "    \n",
    "    for idx1 in y[y[col1]==1].index:\n",
    "        for idx2 in y[y[col2]==1].index:\n",
    "            sims.append(similarities.vector_similarity(X.loc[idx1], X.loc[idx2]))\n",
    "    \n",
    "    return sum(sims)/len(sims)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity_matrix(X, y, sim_method='cosine'):\n",
    "    \n",
    "    import similarities\n",
    "    \n",
    "    sim_df = pd.DataFrame(index=y.columns, columns=y.columns)\n",
    "    \n",
    "    for col in y.columns:\n",
    "    \n",
    "        indexes = y[y[col]==1].index\n",
    "        sim_df.loc[col, col] = similarities.calculate_within_class_similarity(X.loc[indexes])\n",
    "    \n",
    "    for col1, col2 in list(combinations(y.columns, 2)):\n",
    "        sim_df.loc[col1, col2] = calculate_between_class_similarities(col1, col2, X, y)\n",
    "    \n",
    "    return sim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "calculate_within_class_similarity() missing 1 required positional argument: 'sim_calculation_type'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m embedding_method \u001b[38;5;129;01min\u001b[39;00m all_sentence_embeddings:\n\u001b[0;32m     13\u001b[0m     X_num \u001b[38;5;241m=\u001b[39m utilities\u001b[38;5;241m.\u001b[39mvectorize_data(X, embedding_method)\n\u001b[1;32m---> 14\u001b[0m     sim_df \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_similarity_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     embedding_similarities\u001b[38;5;241m.\u001b[39mappend((data, embedding_method, sim_df))\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mcalculate_similarity_matrix\u001b[1;34m(X, y, sim_method)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m y\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m      9\u001b[0m     indexes \u001b[38;5;241m=\u001b[39m y[y[col]\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mindex\n\u001b[1;32m---> 10\u001b[0m     sim_df\u001b[38;5;241m.\u001b[39mloc[col, col] \u001b[38;5;241m=\u001b[39m \u001b[43msimilarities\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_within_class_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindexes\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col1, col2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(combinations(y\u001b[38;5;241m.\u001b[39mcolumns, \u001b[38;5;241m2\u001b[39m)):\n\u001b[0;32m     13\u001b[0m     sim_df\u001b[38;5;241m.\u001b[39mloc[col1, col2] \u001b[38;5;241m=\u001b[39m calculate_between_class_similarities(col1, col2, X, y)\n",
      "\u001b[1;31mTypeError\u001b[0m: calculate_within_class_similarity() missing 1 required positional argument: 'sim_calculation_type'"
     ]
    }
   ],
   "source": [
    "embedding_similarities = [] \n",
    "    \n",
    "for data, path in data_paths.items():    \n",
    "\n",
    "    df = utilities.read_data(path)\n",
    "\n",
    "    X = df['text']\n",
    "    y = df.drop(['text'], axis=1)\n",
    "    X = X.apply(preprocess.preprocess_text)\n",
    "\n",
    "    for embedding_method in all_sentence_embeddings:\n",
    "\n",
    "        X_num = utilities.vectorize_data(X, embedding_method)\n",
    "        sim_df = calculate_similarity_matrix(X_num, y)\n",
    "\n",
    "        embedding_similarities.append((data, embedding_method, sim_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dict = {}\n",
    "for data, embedding_method, sim_df in embedding_similarities:\n",
    "    graph_dict[(data, embedding_method)] = sim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_matrix_score(sim_df):\n",
    "    scores = []\n",
    "    for col in sim_df.columns:\n",
    "        scores.append((sim_df.loc[col, col] - sim_df.loc[col].drop(col).max())/sim_df.loc[col, col])\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for data in data_paths.keys():\n",
    "    print(data)\n",
    "    for embedding_method in sentence_embeddings:\n",
    "        sim_df = graph_dict[(data, embedding_method)]\n",
    "        \n",
    "        scores = calculate_matrix_score(sim_df)\n",
    "        \n",
    "        print(embedding_method)\n",
    "        print(f'max: {max(scores)}, min: {min(scores)}, avg: {sum(scores)/len(scores)}')\n",
    "    print('*'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for data in data_paths.keys():\n",
    "    for embedding_method in sentence_embeddings:\n",
    "        \n",
    "        sim_df = graph_dict[(data, embedding_method)]\n",
    "        \n",
    "        plt.figure()\n",
    "        \n",
    "        sns.heatmap(sim_df.fillna(0), annot=True,\n",
    "        xticklabels=sim_df.columns,\n",
    "        yticklabels=sim_df.columns, cmap=\"rocket_r\", ax=ax1)\n",
    "        ax.set_title(f'{data}, {embedding_method}')\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(\\\"nlpaueb/legal-bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"saibo/legal-roberta-base\")\n",
    "                                  \n",
    "tokenizer = AutoTokenizer.from_pretrained('saibo/legal-roberta-base')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = 'I really love to play football'\n",
    "sentence2 = 'Playing football is my passion.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_sentence(sentence1, sentence2, model, preprocessing=False):\n",
    "    \n",
    "    model = SentenceTransformer(model)\n",
    "    \n",
    "    if preprocessing:\n",
    "        import preprocess\n",
    "        sentence1 = preprocess.preprocess_text(sentence1)\n",
    "        sentence2 = preprocess.preprocess_text(sentence2)\n",
    "        \n",
    "    embedding1 = model.encode(sentence1, convert_to_tensor=True)\n",
    "    embedding2 = model.encode(sentence2, convert_to_tensor=True)\n",
    "    \n",
    "    cosine_scores = util.pytorch_cos_sim(embedding1, embedding2)\n",
    "    \n",
    "    return cosine_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \n",
    "'saibo/legal-roberta-base'\n",
    "'nlpaueb/legal-bert-base-uncased'\n",
    "'nlpaueb/legal-bert-small-uncased'\n",
    "'saibo/legal-roberta-base'\n",
    "'albert-base-v2'\n",
    "'ALBERT-xlarge'\n",
    "'ALBERT-xxlarg'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)    \n",
    "text_tensor1 = tokenizer.encode(sentence1, padding=True, truncation=True, return_tensors='pt')\n",
    "text_tensor1 = tokenizer.encode(sentence2, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    output1 = model(text_tensor1)\n",
    "    output2 = model(text_tensor2)\n",
    "\n",
    "sentence_embeddings1 = mean_pooling(output1, text_tensor1)\n",
    "sentence_embeddings2 = mean_pooling(output2, text_tensor2)\n",
    "\n",
    "cosine_scores = util.pytorch_cos_sim(sentence_embeddings1, sentence_embeddings2)\n",
    "\n",
    "print(cosine_scores.item())\n",
    "\n",
    "print(sentence_embeddings1.shape, sentence_embeddings2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('stsb-roberta-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = 'I love to play football because I am a player'\n",
    "sentence2 = 'Playing football is my passion.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding1 = model.encode(sentence1, convert_to_tensor=True)\n",
    "embedding2 = model.encode(sentence2, convert_to_tensor=True)\n",
    "cosine_scores = util.pytorch_cos_sim(embedding1, embedding2)\n",
    "print(cosine_scores.item())\n",
    "\n",
    "embedding1 = model.encode(preprocess.preprocess_text(sentence1), convert_to_tensor=True)\n",
    "embedding2 = model.encode(preprocess.preprocess_text(sentence2), convert_to_tensor=True)\n",
    "cosine_scores = util.pytorch_cos_sim(embedding1, embedding2)\n",
    "print(cosine_scores.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://medium.com/nlplanet/two-minutes-nlp-11-word-embeddings-models-you-should-know-a0581763b9a9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"http://www.scielo.org.mx/scielo.php?script=sci_arttext&pid=S1405-55462016000400647\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import sub\n",
    "from gensim.utils import simple_preprocess\n",
    "import gensim.downloader as api\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.similarities import WordEmbeddingSimilarityIndex\n",
    "from gensim.similarities import SparseTermSimilarityMatrix\n",
    "from gensim.similarities import SoftCosineSimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_string = 'fruit and vegetables'\n",
    "documents = ['cars drive on the road', 'tomatoes are actually fruit']\n",
    "\n",
    "stopwords = ['the', 'and', 'are', 'a']\n",
    "\n",
    "# From: https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/soft_cosine_tutorial.ipynb\n",
    "def preprocess(doc):\n",
    "    # Tokenize, clean up input document string\n",
    "    doc = sub(r'<img[^<>]+(>|$)', \" image_token \", doc)\n",
    "    doc = sub(r'<[^<>]+(>|$)', \" \", doc)\n",
    "    doc = sub(r'\\[img_assist[^]]*?\\]', \" \", doc)\n",
    "    doc = sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', \" url_token \", doc)\n",
    "    return [token for token in simple_preprocess(doc, min_len=0, max_len=float(\"inf\")) if token not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_string = 'fruit and vegetables'\n",
    "documents = ['cars drive on the road', 'tomatoes are actually fruit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\"I like Python because I can build AI applications\",\n",
    "          \"I like Python because I can do data analytics\",\n",
    "          \"The cat sits on the ground\",\n",
    "         \"The cat walks on the sidewalk\"]\n",
    "\n",
    "query_string = \"I like Javascript because I can build web applications\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the documents, including the query string\n",
    "corpus = [preprocess(document) for document in documents]\n",
    "query = preprocess(query_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model: this is a big file, can take a while to download and open\n",
    "glove = api.load(\"glove-wiki-gigaword-50\")    \n",
    "similarity_index = WordEmbeddingSimilarityIndex(glove)\n",
    "\n",
    "# Build the term dictionary, TF-idf model\n",
    "dictionary = Dictionary(corpus+[query])\n",
    "tfidf = TfidfModel(dictionary=dictionary)\n",
    "\n",
    "# Create the term similarity matrix.  \n",
    "similarity_matrix = SparseTermSimilarityMatrix(similarity_index, dictionary, tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Soft Cosine Measure between the query and the documents.\n",
    "# From: https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/soft_cosine_tutorial.ipynb\n",
    "query_tf = tfidf[dictionary.doc2bow(query)]\n",
    "\n",
    "index = SoftCosineSimilarity(\n",
    "            tfidf[[dictionary.doc2bow(document) for document in corpus]],\n",
    "            similarity_matrix)\n",
    "\n",
    "doc_similarity_scores = index[query_tf]\n",
    "\n",
    "# Output the sorted similarity scores and documents\n",
    "sorted_indexes = np.argsort(doc_similarity_scores)[::-1]\n",
    "for idx in sorted_indexes:\n",
    "    print(f'{idx} \\t {doc_similarity_scores[idx]:0.3f} \\t {documents[idx]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "http://www.scielo.org.mx/scielo.php?script=sci_arttext&pid=S1405-55462016000400647"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
