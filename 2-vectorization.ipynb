{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessing\n",
    "vectorization - converting text to numeric\n",
    "    tfidf\n",
    "    Word2vec\n",
    "    glove\n",
    "    Bert (https://www.kdnuggets.com/2022/01/finetuning-bert-tweets-classification-ft-hugging-face.html)\n",
    "    Fastext\n",
    "    Elmo\n",
    "    XLNet\n",
    "    Transformers (https://pub.towardsai.net/text-classification-using-transformers-a2c6b3395ce3)\n",
    "Reduce the dimensions\n",
    "    Principal Component Analysis\n",
    "    Singular Value Decomposition\n",
    "    Latent Semantic Indexing\n",
    "    Pooling\n",
    "    multi-dimensional scaling\n",
    "calculate imbalanced ratio and find an oversampling ratio\n",
    "oversample the imbalanced dataset using unlabeled data\n",
    "    define a similarity function\n",
    "(extension) define a strategy to choose unlabeled instances wisely to reduce computation time\n",
    "    clustering?\n",
    "    LDA\n",
    "    Non-negative Matrix Factorization\n",
    "    K-NN based graph approach (2 paper’s approaches)\n",
    "it should be trained to maximize the within-class similarity while minimizing between-class similarity using labeled data using the labeled data\n",
    "using the defined similarity function calculate similarity for each unlabeled instance of each class\n",
    "define a similarity function\n",
    "similarity functions for vectors\n",
    "    Measurement of Text Similarity: A Survey: a very detailed survey of similarity functions that are used for text data\n",
    "    cosine similarity\n",
    "    minkowski family (euclidean, manhattan)\n",
    "    hamming distance\n",
    "    Jaccard index\n",
    "    Sorensen-dice index\n",
    "    KL divergence\n",
    "    Jensen–Shannon divergence with LDA\n",
    "    Wasserstein distance\n",
    "    SMTP \n",
    "    word mover’s distance\n",
    "to each labeled instance (possible to create a confidence)\n",
    "take the average of the similarity\n",
    "take the min max average and std and decide using these\n",
    "to each class centroids (generalization)\n",
    "use all instances\n",
    "use only the instances that are similar to each other to ensure data confidence (excluding outliers)\n",
    "if the similarity exceeds some defined threshold assign them to related classes\n",
    "train a classifier and look for an improvement\n",
    "\n",
    "clustering approach\n",
    "does one vs rest classifier work?\n",
    "if it works use self-supervised methods\n",
    "Train a final classifier with oversampled data\n",
    "\n",
    "Experimentation with different datasets\n",
    "ANOVA for parameters to select important parameters\n",
    "Tune important parameters to enhance performance\n",
    "Compare the two cases to show the improvement\n",
    "Compare with different solutions?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import preprocess\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios:\n",
    "    - preprocessing\n",
    "        var\n",
    "        yok\n",
    "        different preprocessing\n",
    "            only remove noise\n",
    "            do not lower\n",
    "    - vectorizers\n",
    "    - similarity functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths = {'opp115'   : r'C:\\Users\\IsmailKaraman\\workspace\\data\\privacy_policy_data\\OPP-115_v2\\majority.csv',\n",
    "              'ohsumed'  : r'C:\\Users\\IsmailKaraman\\workspace\\GitHub\\thesis\\data\\ohsumed.csv',\n",
    "              'reuters'  : r'C:\\Users\\IsmailKaraman\\workspace\\GitHub\\thesis\\data\\Reuters21578.csv'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings =  ['all-MiniLM-L6-v2',\n",
    "                        'all-MiniLM-L12-v2',\n",
    "                        'all-mpnet-base-v2',\n",
    "                        'all-distilroberta-v1',\n",
    "                        'bert-base-nli-mean-tokens',\n",
    "                        'distiluse-base-multilingual-cased-v1',\n",
    "                        'multi-qa-mpnet-base-dot-v1',\n",
    "                        'paraphrase-MiniLM-L6-v2',\n",
    "                        'paraphrase-multilingual-MiniLM-L12-v2',\n",
    "                        'paraphrase-mpnet-base-v2',\n",
    "                        'paraphrase-xlm-r-multilingual-v1',\n",
    "                        'sentence-t5-large',\n",
    "                        'stsb-roberta-large']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = utilities.read_data(data_paths['opp115'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text']\n",
    "y = df.drop(['text'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_between_class_similarities(col1, col2, X, y):\n",
    "    \n",
    "    similarities = []\n",
    "    \n",
    "    for idx1 in y[y[col1]==1].index:\n",
    "        for idx2 in y[y[col2]==1].index:\n",
    "            similarities.append(vector_similarity(X.loc[idx1], X.loc[idx2]))\n",
    "    \n",
    "    return sum(similarities)/len(similarities)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity_matrix(X, y, sim_method):\n",
    "    \n",
    "    import similarities\n",
    "    sim_df = pd.DataFrame(index=y.columns, columns=y.columns)\n",
    "    \n",
    "    for label in y.columns:\n",
    "    \n",
    "        indexes = y[y[label]==1].index\n",
    "        sim_dict.loc[col1, col2] = similarities.calculate_within_class_similarity(X.loc[indexes], sim_method)\n",
    "    \n",
    "    for col1, col2 in list(combinations(y.columns, 2)):\n",
    "        sim_dict.loc[col1, col2] = calculate_between_class_similarities(col1, col2, X, y)\n",
    "        \n",
    "    \n",
    "    for (col1,col2), sim in similarities.items():\n",
    "        sim_df.loc[col1, col2] =  float(sim)\n",
    "    \n",
    "    return sim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for embedding_method in sentence_embeddings:\n",
    "    \n",
    "    X_num = utilities.vectorize_data(X, embedding_method)\n",
    "    X_num = pd.Series([np.squeeze(i) for i in X_num])\n",
    "    \n",
    "    sim_df = calculate_similarity_matrix(X_num, y)\n",
    "    \n",
    "    import seaborn as sns\n",
    "\n",
    "    sns.heatmap(sim_df.fillna(0), annot=True,\n",
    "    xticklabels=sim_df.columns,\n",
    "    yticklabels=sim_df.columns, cmap=\"BuPu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = AutoModel.from_pretrained(\\\"nlpaueb/legal-bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"saibo/legal-roberta-base\")\n",
    "                                  \n",
    "tokenizer = AutoTokenizer.from_pretrained('saibo/legal-roberta-base')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-38-4a341962c415>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-38-4a341962c415>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    save sim_df adnd run again to compare results\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "adasd\n",
    "\n",
    "save sim_df adnd run again to compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = 'I really love to play football'\n",
    "sentence2 = 'Playing football is my passion.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_sentence(sentence1, sentence2, model, preprocessing=False):\n",
    "    \n",
    "    model = SentenceTransformer(model)\n",
    "    \n",
    "    if preprocessing:\n",
    "        import preprocess\n",
    "        sentence1 = preprocess.preprocess_text(sentence1)\n",
    "        sentence2 = preprocess.preprocess_text(sentence2)\n",
    "        \n",
    "    embedding1 = model.encode(sentence1, convert_to_tensor=True)\n",
    "    embedding2 = model.encode(sentence2, convert_to_tensor=True)\n",
    "    \n",
    "    cosine_scores = util.pytorch_cos_sim(embedding1, embedding2)\n",
    "    \n",
    "    return cosine_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# multi-qa-mpnet-base-dot-v1 - https://huggingface.co/sentence-transformers/multi-qa-mpnet-base-dot-v1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \n",
    "'saibo/legal-roberta-base'\n",
    "'nlpaueb/legal-bert-base-uncased'\n",
    "'nlpaueb/legal-bert-small-uncased'\n",
    "'saibo/legal-roberta-base'\n",
    "'albert-base-v2'\n",
    "'ALBERT-xlarge'\n",
    "'ALBERT-xxlarg'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)    \n",
    "text_tensor1 = tokenizer.encode(sentence1, padding=True, truncation=True, return_tensors='pt')\n",
    "text_tensor1 = tokenizer.encode(sentence2, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    output1 = model(text_tensor1)\n",
    "    output2 = model(text_tensor2)\n",
    "\n",
    "sentence_embeddings1 = mean_pooling(output1, text_tensor1)\n",
    "sentence_embeddings2 = mean_pooling(output2, text_tensor2)\n",
    "\n",
    "cosine_scores = util.pytorch_cos_sim(sentence_embeddings1, sentence_embeddings2)\n",
    "\n",
    "print(cosine_scores.item())\n",
    "\n",
    "print(sentence_embeddings1.shape, sentence_embeddings2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('stsb-roberta-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = 'I love to play football because I am a player'\n",
    "sentence2 = 'Playing football is my passion.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding1 = model.encode(sentence1, convert_to_tensor=True)\n",
    "embedding2 = model.encode(sentence2, convert_to_tensor=True)\n",
    "cosine_scores = util.pytorch_cos_sim(embedding1, embedding2)\n",
    "print(cosine_scores.item())\n",
    "\n",
    "embedding1 = model.encode(preprocess.preprocess_text(sentence1), convert_to_tensor=True)\n",
    "embedding2 = model.encode(preprocess.preprocess_text(sentence2), convert_to_tensor=True)\n",
    "cosine_scores = util.pytorch_cos_sim(embedding1, embedding2)\n",
    "print(cosine_scores.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://medium.com/nlplanet/two-minutes-nlp-11-word-embeddings-models-you-should-know-a0581763b9a9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"http://www.scielo.org.mx/scielo.php?script=sci_arttext&pid=S1405-55462016000400647\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import sub\n",
    "from gensim.utils import simple_preprocess\n",
    "import gensim.downloader as api\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.similarities import WordEmbeddingSimilarityIndex\n",
    "from gensim.similarities import SparseTermSimilarityMatrix\n",
    "from gensim.similarities import SoftCosineSimilarity\n",
    "\n",
    "query_string = 'fruit and vegetables'\n",
    "documents = ['cars drive on the road', 'tomatoes are actually fruit']\n",
    "stopwords = ['the', 'and', 'are', 'a']\n",
    "\n",
    "# From: https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/soft_cosine_tutorial.ipynb\n",
    "def preprocess(doc):\n",
    "    # Tokenize, clean up input document string\n",
    "    doc = sub(r'<img[^<>]+(>|$)',  image_token  doc)\n",
    "    doc = sub(r'<[^<>]+(>|$)',   doc)\n",
    "    doc = sub(r'\\[img_assist[^]]*?\\]',   doc)\n",
    "    doc = sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',  url_token  doc)\n",
    "    return [token for token in simple_preprocess(doc, min_len=0, max_len=float(inf)) if token not in stopwords]\n",
    "\n",
    "query_string = 'fruit and vegetables'\n",
    "documents = ['cars drive on the road', 'tomatoes are actually fruit']\n",
    "documents = ['I like Python because I can build AI applications',\n",
    "             'I like Python because I can do data analytics'\n",
    "             'The cat sits on the ground',\n",
    "             'The cat walks on the sidewalk']\n",
    "\n",
    "query_string = 'I like Javascript because I can build web applications'\n",
    "# Preprocess the documents, including the query string\n",
    "\n",
    "corpus = [preprocess(document) for document in documents]\n",
    "query = preprocess(query_string)\n",
    "# Load the model: this is a big file, can take a while to download and open\n",
    "glove = api.load(glove-wiki-gigaword-50)    \n",
    "similarity_index = WordEmbeddingSimilarityIndex(glove)\n",
    "\n",
    "# Build the term dictionary, TF-idf model\n",
    "dictionary = Dictionary(corpus+[query])\n",
    "tfidf = TfidfModel(dictionary=dictionary)\n",
    "\n",
    "# Create the term similarity matrix.  \n",
    "similarity_matrix = SparseTermSimilarityMatrix(similarity_index, dictionary, tfidf)\n",
    "                # Compute Soft Cosine Measure between the query and the documents.\n",
    "# From: https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/soft_cosine_tutorial.ipynb\n",
    "query_tf = tfidf[dictionary.doc2bow(query)]\n",
    "index = SoftCosineSimilarity(\n",
    "            tfidf[[dictionary.doc2bow(document) for document in corpus]],\n",
    "\n",
    "            similarity_matrix)\n",
    "\n",
    "doc_similarity_scores = index[query_tf]\n",
    "\n",
    "# Output the sorted similarity scores and documents\n",
    "\n",
    "sorted_indexes = np.argsort(doc_similarity_scores)[::-1]\n",
    "\n",
    "for idx in sorted_indexes:\n",
    "\n",
    "    print(f'{idx} \\t {doc_similarity_scores[idx]:0.3f} \\t {documents[idx]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# context based models\n",
    "source: https://medium.com/nlplanet/two-minutes-nlp-11-word-embeddings-models-you-should-know-a0581763b9a9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo\n",
    "cove\n",
    "xlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### fine-tuning sentence-embedding models\n",
    "\n",
    "it is possible to fine-tune sentence similarity models by using cosine similarity loss function. \n",
    "some pair of sentences and similarity between them are given as training data and models can be fine-tuned\n",
    "by using in-class similarity as high score and between-class similarity as low score we can tune the model to find more instances by the similarity\n",
    "\n",
    "https://www.sbert.net/docs/training/overview.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import sub\n",
    "from gensim.utils import simple_preprocess\n",
    "import gensim.downloader as api\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.similarities import WordEmbeddingSimilarityIndex\n",
    "from gensim.similarities import SparseTermSimilarityMatrix\n",
    "from gensim.similarities import SoftCosineSimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_string = 'fruit and vegetables'\n",
    "documents = ['cars drive on the road', 'tomatoes are actually fruit']\n",
    "\n",
    "stopwords = ['the', 'and', 'are', 'a']\n",
    "\n",
    "# From: https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/soft_cosine_tutorial.ipynb\n",
    "def preprocess(doc):\n",
    "    # Tokenize, clean up input document string\n",
    "    doc = sub(r'<img[^<>]+(>|$)', \" image_token \", doc)\n",
    "    doc = sub(r'<[^<>]+(>|$)', \" \", doc)\n",
    "    doc = sub(r'\\[img_assist[^]]*?\\]', \" \", doc)\n",
    "    doc = sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', \" url_token \", doc)\n",
    "    return [token for token in simple_preprocess(doc, min_len=0, max_len=float(\"inf\")) if token not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_string = 'fruit and vegetables'\n",
    "documents = ['cars drive on the road', 'tomatoes are actually fruit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\"I like Python because I can build AI applications\",\n",
    "          \"I like Python because I can do data analytics\",\n",
    "          \"The cat sits on the ground\",\n",
    "         \"The cat walks on the sidewalk\"]\n",
    "\n",
    "query_string = \"I like Javascript because I can build web applications\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the documents, including the query string\n",
    "corpus = [preprocess(document) for document in documents]\n",
    "query = preprocess(query_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model: this is a big file, can take a while to download and open\n",
    "glove = api.load(\"glove-wiki-gigaword-50\")    \n",
    "similarity_index = WordEmbeddingSimilarityIndex(glove)\n",
    "\n",
    "# Build the term dictionary, TF-idf model\n",
    "dictionary = Dictionary(corpus+[query])\n",
    "tfidf = TfidfModel(dictionary=dictionary)\n",
    "\n",
    "# Create the term similarity matrix.  \n",
    "similarity_matrix = SparseTermSimilarityMatrix(similarity_index, dictionary, tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Soft Cosine Measure between the query and the documents.\n",
    "# From: https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/soft_cosine_tutorial.ipynb\n",
    "query_tf = tfidf[dictionary.doc2bow(query)]\n",
    "\n",
    "index = SoftCosineSimilarity(\n",
    "            tfidf[[dictionary.doc2bow(document) for document in corpus]],\n",
    "            similarity_matrix)\n",
    "\n",
    "doc_similarity_scores = index[query_tf]\n",
    "\n",
    "# Output the sorted similarity scores and documents\n",
    "sorted_indexes = np.argsort(doc_similarity_scores)[::-1]\n",
    "for idx in sorted_indexes:\n",
    "    print(f'{idx} \\t {doc_similarity_scores[idx]:0.3f} \\t {documents[idx]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "http://www.scielo.org.mx/scielo.php?script=sci_arttext&pid=S1405-55462016000400647"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
