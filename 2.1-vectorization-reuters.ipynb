{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "referring for articles:\n",
    "https://www.sbert.net/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import preprocess\n",
    "import utilities\n",
    "import similarities\n",
    "import parameters\n",
    "import seaborn as sns\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths = parameters.data_paths\n",
    "sim_calculation_type = parameters.sim_calculation_type\n",
    "all_sentence_embeddings =  parameters.huggingface_embeddings + parameters.openai_embeddings + parameters.google_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_within_class(X, y, sim_calculation_type, sim_df):\n",
    "    \n",
    "    import ray\n",
    "    ray.init(num_cpus=3, ignore_reinit_error=True)\n",
    "    \n",
    "    @ray.remote\n",
    "    def run(vecs, sim_calculation_type):\n",
    "        return similarities.calculate_similarity_within_classes(vecs, sim_calculation_type)\n",
    "\n",
    "    futures = [run.remote(X.loc[y[y[col]==1].index], sim_calculation_type) for col in y.columns]\n",
    "    results = ray.get(futures) \n",
    "    \n",
    "    for col, sim in zip(y.columns, results):    \n",
    "        sim_df.loc[col, col] = sim\n",
    "    \n",
    "    ray.shutdown()\n",
    "\n",
    "    return sim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_within_class(X, y, sim_calculation_type, sim_df):\n",
    "                 \n",
    "    max_batch_size = 2_000  \n",
    "    \n",
    "    for col in y.columns:   \n",
    "        \n",
    "        all_sim = [] \n",
    "        \n",
    "        col_indexes = y[y[col]==1].index\n",
    "        batch_num = int(len(col_indexes) / max_batch_size) + (len(col_indexes) % max_batch_size > 0)\n",
    "        \n",
    "        for batch in np.array_split(col_indexes, batch_num):\n",
    "            \n",
    "            all_sim.extend(similarities.calculate_similarity_within_classes(X.loc[batch], sim_calculation_type=None))\n",
    "        \n",
    "        sim_df.loc[col, col] = np.mean(all_sim)\n",
    "        \n",
    "    return sim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_between_class(X, y, sim_df):\n",
    "    \n",
    "    import ray\n",
    "    ray.init(num_cpus=3,ignore_reinit_error=True)\n",
    "    \n",
    "    @ray.remote\n",
    "    def run(vecs1, vecs2):\n",
    "        return similarities.calculate_similarity_between_classes(vecs1, vecs2, sim_calculation_type)\n",
    "    \n",
    "    futures = [run.remote(X[y[y[col1]==1].index], X[y[y[col2]==1].index]) for col1, col2 in list(combinations(y.columns, 2))]\n",
    "    results = ray.get(futures) \n",
    "    \n",
    "    for (col1, col2), sim in zip(list(combinations(y.columns, 2)), results):\n",
    "        sim_df.loc[col1, col2] = sim\n",
    "    \n",
    "    ray.shutdown()\n",
    "    \n",
    "    return sim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_between_class(X, y, sim_df):\n",
    "    \n",
    "    max_batch_size = 2_000\n",
    "    \n",
    "    for col1, col2 in list(combinations(y.columns, 2)):\n",
    "        \n",
    "        all_sim = [] \n",
    "        \n",
    "        col1_indexes = y[y[col1]==1].index\n",
    "        col2_indexes = y[y[col2]==1].index\n",
    "        \n",
    "        batch_num1 = int(len(col1_indexes) / max_batch_size) + (len(col1_indexes) % max_batch_size > 0)\n",
    "        batch_num2 = int(len(col2_indexes) / max_batch_size) + (len(col2_indexes) % max_batch_size > 0)\n",
    "        \n",
    "        for batch1 in np.array_split(col1_indexes, batch_num1):\n",
    "            for batch2 in np.array_split(col2_indexes, batch_num2):\n",
    "                \n",
    "                all_sim.extend(similarities.calculate_similarity_between_classes(X.loc[batch1], X.loc[batch2], sim_calculation_type=None))\n",
    "        \n",
    "        sim_df.loc[col1, col2] = np.mean(all_sim)\n",
    "        \n",
    "    return sim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity_matrix(X, y, sim_method='cosine'):\n",
    "    \n",
    "    sim_df = pd.DataFrame(index=y.columns, columns=y.columns)\n",
    "    sim_df = calculate_within_class(X, y, sim_calculation_type, sim_df)\n",
    "    sim_df = calculate_between_class(X, y, sim_df)\n",
    "    \n",
    "    return sim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_file = 'embedding_results_reuters.p'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pickle\n",
    "results = {}\n",
    "with open(res_file, 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/31 [00:05<?, ?it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = 'reuters'\n",
    "path = data_paths[data]\n",
    "\n",
    "df = utilities.read_data(path)\n",
    "X = df['text']\n",
    "y = df.drop(['text'], axis=1)\n",
    "X = X.apply(preprocess.preprocess_text)\n",
    "\n",
    "import pickle\n",
    "with open(res_file, 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "iterate = [i for i in all_sentence_embeddings if i not in results.keys()]\n",
    "results['failed_embedings'] = []\n",
    "\n",
    "for embedding_method in tqdm(iterate):\n",
    "    \n",
    "    try:\n",
    "        X_num = utilities.vectorize_data(X, embedding_method)\n",
    "        sim_df = calculate_similarity_matrix(X_num, y)\n",
    "        results[embedding_method] = sim_df\n",
    "        print(embedding_method, ' completed ... ')\n",
    "    except Exception as e: \n",
    "        print(f'! {embedding_method} failed due to {e}... ')\n",
    "        results['failed_embedings'].append(embedding_method)\n",
    "    \n",
    "    with open(res_file, 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "        \n",
    "    import torch, gc\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['failed_embedings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(res_file, 'rb') as f:\n",
    "    results_opp115 = pickle.load(f)\n",
    "results.pop('failed_embedings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_matrix_score(sim_df):\n",
    "    scores = []\n",
    "    for col in sim_df.columns:\n",
    "        scores.append((sim_df.loc[col, col] - sim_df.loc[col].drop(col).max())/sim_df.loc[col, col])\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best:\n",
    "'paraphrase-MiniLM-L6-v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed = ['stsb-roberta-large',\n",
    "           'albert-base-v2',\n",
    "           'bert-base-nli-mean-tokens',\n",
    "           'bert-base-uncased',\n",
    "           'distilbert-base-nli-mean-tokens',\n",
    "           'nlpaueb/legal-bert-base-uncased',\n",
    "           'saibo/legal-roberta-base',\n",
    "           'sentence-t5-large',\n",
    "           'sentence-transformers/average_word_embeddings_glove.6B.300d',\n",
    "           'sentence-transformers/average_word_embeddings_glove.840B.300d',\n",
    "           'distiluse-base-multilingual-cased-v1',\n",
    "           'multi-qa-mpnet-base-dot-v1',\n",
    "           'paraphrase-mpnet-base-v2',\n",
    "           'paraphrase-multilingual-MiniLM-L12-v2',\n",
    "           'paraphrase-MiniLM-L6-v2',\n",
    "           'paraphrase-xlm-r-multilingual-v1',\n",
    "           'universal-sentence-encoder'\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = results.copy()\n",
    "finel_res = {v:k for v, k in res.items() if v not in removed}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for embedding, sim_df in finel_res.items():\n",
    "    scores = calculate_matrix_score(sim_df)\n",
    "    scores = scores[:-1]\n",
    "    print(f'{embedding} --- max: {max(scores):.2f}, min: {min(scores):.2f}, avg: {sum(scores)/len(scores):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for embedding, sim_df in finel_res.items():\n",
    "    \n",
    "    plt.figure(i, figsize=(8,5))\n",
    "    plt.title(f'{embedding}')\n",
    "    sns.heatmap(sim_df.fillna(0), annot=True,\n",
    "    xticklabels=sim_df.columns,\n",
    "    yticklabels=sim_df.columns, cmap=\"rocket_r\")\n",
    "    \n",
    "\n",
    "    plt.show()\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finel_res.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
